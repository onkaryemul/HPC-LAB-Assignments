{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Parallel Programming using CUDA C**"
      ],
      "metadata": {
        "id": "S3A5wglbg849"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem Statement 1: Vector Addition using CUDA**\n",
        "\n",
        "**Problem Statement:**\n",
        "\n",
        "**Write a CUDA C program that performs element-wise addition of two vectors A and B of size N. The result of the addition should be stored in vector C.**\n",
        "\n",
        "Details:\n",
        "1. Initialize the vectors A and B with random numbers.\n",
        "2. The output vector C[i] = A[i] + B[i], where i ranges from 0 to N-1.\n",
        "3. Use CUDA kernels to perform the computation in parallel.\n",
        "4. Write the code for both serial (CPU-based) and parallel (CUDA-based) implementations.\n",
        "5. Measure the execution time of both the serial and CUDA implementations for different values of N (e.g., N = 10^5, 10^6, 10^7).\n",
        "\n",
        "Task:\n",
        "\n",
        "   • Calculate and report the speedup (i.e., the ratio of CPU execution time to GPU execution time)."
      ],
      "metadata": {
        "id": "Z2ArscHCZXlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vector_add_cuda.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "\n",
        "// CUDA kernel for vector addition\n",
        "__global__ void vectorAddCUDA(float *A, float *B, float *C, int N) {\n",
        "    int i = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    if (i < N) {\n",
        "        C[i] = A[i] + B[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    if (argc != 2) {\n",
        "        fprintf(stderr, \"Usage: %s <vector_size>\\n\", argv[0]);\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    int N = atoi(argv[1]);  // Get vector size from command line argument\n",
        "    if (N <= 0) {\n",
        "        fprintf(stderr, \"Error: vector size must be a positive integer.\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    size_t size = N * sizeof(float);\n",
        "\n",
        "    // Allocate memory on host\n",
        "    float *h_A = (float *)malloc(size);\n",
        "    float *h_B = (float *)malloc(size);\n",
        "    float *h_C = (float *)malloc(size);\n",
        "\n",
        "    // Initialize vectors with random values\n",
        "    srand(time(NULL));  // Seed the random number generator\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        h_A[i] = rand() % 100;\n",
        "        h_B[i] = rand() % 100;\n",
        "    }\n",
        "\n",
        "    // Allocate memory on device (GPU)\n",
        "    float *d_A, *d_B, *d_C;\n",
        "    cudaMalloc(&d_A, size);\n",
        "    cudaMalloc(&d_B, size);\n",
        "    cudaMalloc(&d_C, size);\n",
        "\n",
        "    // Copy vectors from host to device\n",
        "    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Measure time for GPU execution\n",
        "    clock_t start = clock();\n",
        "\n",
        "    // Define grid and block sizes\n",
        "    int blockSize = 256;\n",
        "    int numBlocks = (N + blockSize - 1) / blockSize;\n",
        "\n",
        "    // Launch kernel\n",
        "    vectorAddCUDA<<<numBlocks, blockSize>>>(d_A, d_B, d_C, N);\n",
        "\n",
        "    // Synchronize the device\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    clock_t end = clock();\n",
        "\n",
        "    // GPU execution time\n",
        "    double gpu_time = (double)(end - start) / CLOCKS_PER_SEC;\n",
        "    printf(\"GPU execution time: %f seconds\\n\", gpu_time);\n",
        "\n",
        "    // Copy result from device to host\n",
        "    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "\n",
        "    // Free host memory\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnGAQ2RlZbLB",
        "outputId": "8dd989c0-0787-4b3a-a008-241b0f04525a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vector_add_cuda.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc vector_add_cuda.cu -o vector_add_cuda"
      ],
      "metadata": {
        "id": "sOmZ8j7BbklE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./vector_add_cuda 100000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LFFruhmcTby",
        "outputId": "dfce05a1-6b85-4f5c-a200-d0c2b9ce823e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU execution time: 0.000201 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./vector_add_cuda 1000000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t85nIASpce4M",
        "outputId": "05b0c8b0-cc94-4be6-94a8-bec2e29a8db6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU execution time: 0.000242 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./vector_add_cuda 10000000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOJgQWgxcfDg",
        "outputId": "6870ba31-985b-4d1f-ae41-c6ae3a1d0b5b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU execution time: 0.000649 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem Statement 2: Matrix Addition using CUDA**\n",
        "\n",
        "**Problem Statement:**\n",
        "\n",
        "**Write a CUDA C program to perform element-wise addition of two matrices A and B of size M x N. The result of the addition should be stored in matrix C.**\n",
        "\n",
        "Details:\n",
        ".\n",
        "1.   Initialize the matrices A and B with random values\n",
        "2.   The output matrix C[i][j] = A[i][j] + B[i][j] where i ranges from 0 to M-1 and j ranges from 0 to N-1.\n",
        "3.   Write code for both serial (CPU-based) and parallel (CUDA-based) implementations.\n",
        "4.   Measure the execution time of both implementations for various matrix sizes (e.g., 100x100, 500x500, 1000x1000).\n",
        "\n",
        "Task:\n",
        "\n",
        "• Calculate the speedup using the execution times of the CPU and GPU implementations."
      ],
      "metadata": {
        "id": "EBJL9Or2eOee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrix_add_cuda.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "\n",
        "// CUDA kernel for matrix addition\n",
        "__global__ void matrixAddCUDA(float *A, float *B, float *C, int M, int N) {\n",
        "    int i = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int j = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (i < M && j < N) {\n",
        "        int index = i * N + j;\n",
        "        C[index] = A[index] + B[index];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Helper function to allocate a 2D array on the device\n",
        "float* allocate2DArrayOnDevice(int M, int N) {\n",
        "    float *array;\n",
        "    cudaMalloc((void**)&array, M * N * sizeof(float));\n",
        "    return array;\n",
        "}\n",
        "\n",
        "// Helper function to free a 2D array on the device\n",
        "void free2DArrayOnDevice(float *array) {\n",
        "    cudaFree(array);\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    if (argc != 3) {\n",
        "        fprintf(stderr, \"Usage: %s <rows> <columns>\\n\", argv[0]);\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    int M = atoi(argv[1]);  // Get number of rows from command line argument\n",
        "    int N = atoi(argv[2]);  // Get number of columns from command line argument\n",
        "    if (M <= 0 || N <= 0) {\n",
        "        fprintf(stderr, \"Error: both dimensions must be positive integers.\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    // Allocate memory on host\n",
        "    float *h_A = (float *)malloc(M * N * sizeof(float));\n",
        "    float *h_B = (float *)malloc(M * N * sizeof(float));\n",
        "    float *h_C = (float *)malloc(M * N * sizeof(float));\n",
        "\n",
        "    // Initialize matrices with random values\n",
        "    srand(time(NULL));  // Seed the random number generator\n",
        "    for (int i = 0; i < M * N; i++) {\n",
        "        h_A[i] = rand() % 100;\n",
        "        h_B[i] = rand() % 100;\n",
        "    }\n",
        "\n",
        "    // Allocate memory on device (GPU)\n",
        "    float *d_A = allocate2DArrayOnDevice(M, N);\n",
        "    float *d_B = allocate2DArrayOnDevice(M, N);\n",
        "    float *d_C = allocate2DArrayOnDevice(M, N);\n",
        "\n",
        "    // Copy matrices from host to device\n",
        "    cudaMemcpy(d_A, h_A, M * N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, M * N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Define grid and block sizes\n",
        "    dim3 threadsPerBlock(4, 4);  // 16x16 block size\n",
        "    dim3 numBlocks((N + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                   (M + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "\n",
        "    // Measure time for GPU execution\n",
        "    clock_t start = clock();\n",
        "\n",
        "    // Launch kernel\n",
        "    matrixAddCUDA<<<numBlocks, threadsPerBlock>>>(d_A, d_B, d_C, M, N);\n",
        "\n",
        "    // Synchronize the device\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    clock_t end = clock();\n",
        "\n",
        "    // GPU execution time\n",
        "    double gpu_time = (double)(end - start) / CLOCKS_PER_SEC;\n",
        "    printf(\"GPU execution time: %f seconds\\n\", gpu_time);\n",
        "\n",
        "    // Copy result from device to host\n",
        "    cudaMemcpy(h_C, d_C, M * N * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print a few elements of the result matrix\n",
        "    // printf(\"Result matrix C (first 5 elements):\\n\");\n",
        "    // for (int i = 0; i < 5 && i < M * N; i++) {\n",
        "    //    printf(\"%f \", h_C[i]);\n",
        "    // }\n",
        "    // printf(\"\\n\");\n",
        "\n",
        "    // Free device memory\n",
        "    free2DArrayOnDevice(d_A);\n",
        "    free2DArrayOnDevice(d_B);\n",
        "    free2DArrayOnDevice(d_C);\n",
        "\n",
        "    // Free host memory\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5h9ZEDpcfJp",
        "outputId": "24bb6641-72b4-4016-e0b1-8a8e63b59663"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matrix_add_cuda.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc matrix_add_cuda.cu -o matrix_add_cuda"
      ],
      "metadata": {
        "id": "xcqhtZDxhIVM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./matrix_add_cuda 100 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSL_mvjuhIgd",
        "outputId": "ccc035ce-d574-4463-a1db-9f7d5dc1c8c1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU execution time: 0.000184 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./matrix_add_cuda 500 500"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wS4xeIbXhIjR",
        "outputId": "50d39137-a774-4eeb-d9b5-148182f446a0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU execution time: 0.000224 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./matrix_add_cuda 1000 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iRt3-38hIni",
        "outputId": "e942fc9f-3aa7-4580-c0a0-58ca9505780d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU execution time: 0.000409 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem Statement 3: Dot Product of Two Vectors using CUDA**\n",
        "\n",
        "**Problem Statement:**\n",
        "\n",
        "**Write a CUDA C program to compute the dot product of two vectors A and B of size N. The dot product is defined as:**\n",
        "\n",
        "Details:\n",
        "1. Initialize the vectors A and B with random values.\n",
        "2. Implement the dot product calculation using both serial (CPU) and parallel (CUDA) approaches.\n",
        "3. Measure the execution time for both implementations with different vector sizes (e.g., N = 10^5, 10^6, 10^7).\n",
        "4. Use atomic operations or shared memory reduction in the CUDA kernel to compute the final sum.\n",
        "      \n",
        "Task:\n",
        "\n",
        "• Calculate and report the speedup for different vector sizes."
      ],
      "metadata": {
        "id": "avIuXzkQ2BqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dot_product_cuda.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "\n",
        "// CUDA kernel for dot product using atomic operations\n",
        "__global__ void dotProductCUDA(float *A, float *B, float *C, int N) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < N) {\n",
        "        atomicAdd(C, A[idx] * B[idx]);\n",
        "    }\n",
        "}\n",
        "\n",
        "// Serial dot product calculation for CPU\n",
        "float dotProductCPU(float *A, float *B, int N) {\n",
        "    float sum = 0;\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        sum += A[i] * B[i];\n",
        "    }\n",
        "    return sum;\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    if (argc != 2) {\n",
        "        fprintf(stderr, \"Usage: %s <vector_size>\\n\", argv[0]);\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    int N = atoi(argv[1]);  // Get vector size from command line argument\n",
        "    if (N <= 0) {\n",
        "        fprintf(stderr, \"Error: vector size must be a positive integer.\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    size_t size = N * sizeof(float);\n",
        "\n",
        "    // Allocate memory on host\n",
        "    float *h_A = (float *)malloc(size);\n",
        "    float *h_B = (float *)malloc(size);\n",
        "    float *h_C = (float *)malloc(sizeof(float));\n",
        "\n",
        "    // Initialize vectors with random values\n",
        "    srand(time(NULL));  // Seed the random number generator\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        h_A[i] = rand() % 100;\n",
        "        h_B[i] = rand() % 100;\n",
        "    }\n",
        "    *h_C = 0;  // Initialize result to 0\n",
        "\n",
        "    // Allocate memory on device (GPU)\n",
        "    float *d_A, *d_B, *d_C;\n",
        "    cudaMalloc(&d_A, size);\n",
        "    cudaMalloc(&d_B, size);\n",
        "    cudaMalloc(&d_C, sizeof(float));\n",
        "\n",
        "    // Copy vectors from host to device\n",
        "    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_C, h_C, sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Define grid and block sizes\n",
        "    int blockSize = 256;  // 256 threads per block\n",
        "    int gridSize = (N + blockSize - 1) / blockSize;\n",
        "\n",
        "    // Measure time for GPU execution\n",
        "    clock_t start = clock();\n",
        "\n",
        "    // Launch kernel\n",
        "    dotProductCUDA<<<gridSize, blockSize>>>(d_A, d_B, d_C, N);\n",
        "\n",
        "    // Synchronize the device\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    clock_t end = clock();\n",
        "\n",
        "    // GPU execution time\n",
        "    double gpu_time = (double)(end - start) / CLOCKS_PER_SEC;\n",
        "\n",
        "    // Copy result from device to host\n",
        "    cudaMemcpy(h_C, d_C, sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"GPU Dot Product: %f\\n\", *h_C);\n",
        "    printf(\"GPU execution time: %f seconds\\n\", gpu_time);\n",
        "\n",
        "    // Calculate speedup (assuming CPU result calculation is not included in this code)\n",
        "    // Uncomment below if CPU timing is added\n",
        "    // double speedup = cpu_time / gpu_time;\n",
        "    // printf(\"Speedup (CPU/GPU): %f\\n\", speedup);\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "\n",
        "    // Free host memory\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdjlSpVhcfMs",
        "outputId": "9a4f0878-38a4-4fdd-c985-645c2f1c7794"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dot_product_cuda.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc dot_product_cuda.cu -o dot_product_cuda"
      ],
      "metadata": {
        "id": "6aiPNTrP2hRi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./dot_product_cuda 100000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cuLhzqK2ha0",
        "outputId": "488fd7a3-6140-43fa-b39f-a116fac51fd8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Dot Product: 245451424.000000\n",
            "GPU execution time: 0.055302 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./dot_product_cuda 1000000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaB_eue62hdn",
        "outputId": "e4142fe2-6aa3-4595-b78f-c54f90c6a312"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Dot Product: 2450514432.000000\n",
            "GPU execution time: 0.003724 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./dot_product_cuda 10000000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcu_YSvRhGIm",
        "outputId": "565aff1b-885d-482a-ff15-538ad231ce96"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Dot Product: 24211341312.000000\n",
            "GPU execution time: 0.035262 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem Statement 4: Matrix Multiplication using CUDA**\n",
        "\n",
        "**Problem Statement:**\n",
        "\n",
        "**Write a CUDA C program to perform matrix multiplication. Given two matrices A (MxN) and B (NxP), compute the resulting matrix C (MxP) where:**\n",
        "\n",
        "Details:\n",
        "1. Initialize the matrices A and B with random values.\n",
        "2. Write code for both serial (CPU-based) and parallel (CUDA-based) implementations.\n",
        "3. Measure the execution time of both implementations for various matrix sizes (e.g., 100x100, 500x500, 1000x1000).\n",
        "\n",
        "Task:\n",
        "\n",
        "• Calculate the speedup by comparing the CPU and GPU execution times."
      ],
      "metadata": {
        "id": "D9TMzYro_wki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrix_multiplication_cuda.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "\n",
        "// CUDA kernel for matrix multiplication\n",
        "__global__ void matrixMultiplicationCUDA(float *A, float *B, float *C, int M, int N, int P) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < M && col < P) {\n",
        "        float sum = 0;\n",
        "        for (int k = 0; k < N; k++) {\n",
        "            sum += A[row * N + k] * B[k * P + col];\n",
        "        }\n",
        "        C[row * P + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    if (argc != 4) {\n",
        "        fprintf(stderr, \"Usage: %s <M> <N> <P>\\n\", argv[0]);\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    int M = atoi(argv[1]);  // Rows of A and C\n",
        "    int N = atoi(argv[2]);  // Columns of A and Rows of B\n",
        "    int P = atoi(argv[3]);  // Columns of B and C\n",
        "\n",
        "    if (M <= 0 || N <= 0 || P <= 0) {\n",
        "        fprintf(stderr, \"Error: All dimensions must be positive integers.\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    size_t size_A = M * N * sizeof(float);\n",
        "    size_t size_B = N * P * sizeof(float);\n",
        "    size_t size_C = M * P * sizeof(float);\n",
        "\n",
        "    // Allocate memory on host\n",
        "    float *h_A = (float *)malloc(size_A);\n",
        "    float *h_B = (float *)malloc(size_B);\n",
        "    float *h_C = (float *)malloc(size_C);\n",
        "\n",
        "    // Initialize matrices A and B with random values\n",
        "    srand(time(NULL));  // Seed the random number generator\n",
        "    for (int i = 0; i < M * N; i++) {\n",
        "        h_A[i] = rand() % 100;\n",
        "    }\n",
        "    for (int i = 0; i < N * P; i++) {\n",
        "        h_B[i] = rand() % 100;\n",
        "    }\n",
        "\n",
        "    // Allocate memory on device (GPU)\n",
        "    float *d_A, *d_B, *d_C;\n",
        "    cudaMalloc(&d_A, size_A);\n",
        "    cudaMalloc(&d_B, size_B);\n",
        "    cudaMalloc(&d_C, size_C);\n",
        "\n",
        "    // Copy matrices from host to device\n",
        "    cudaMemcpy(d_A, h_A, size_A, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, size_B, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Define grid and block dimensions\n",
        "    dim3 blockSize(16, 16);  // Block of 16x16 threads\n",
        "    dim3 gridSize((P + blockSize.x - 1) / blockSize.x, (M + blockSize.y - 1) / blockSize.y);\n",
        "\n",
        "    // Measure GPU execution time\n",
        "    clock_t start = clock();\n",
        "\n",
        "    // Launch CUDA kernel\n",
        "    matrixMultiplicationCUDA<<<gridSize, blockSize>>>(d_A, d_B, d_C, M, N, P);\n",
        "\n",
        "    // Synchronize device\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    clock_t end = clock();\n",
        "\n",
        "    double gpu_time = (double)(end - start) / CLOCKS_PER_SEC;\n",
        "    printf(\"GPU execution time: %f seconds\\n\", gpu_time);\n",
        "\n",
        "    // Copy result from device to host\n",
        "    cudaMemcpy(h_C, d_C, size_C, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "\n",
        "    // Free host memory\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFuzhLKYcfPg",
        "outputId": "4c1265da-fc9e-486a-8cbc-f341eaa94f43"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrix_multiplication_cuda.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc matrix_multiplication_cuda.cu -o matrix_multiplication_cuda"
      ],
      "metadata": {
        "id": "OyZyVy3n9jlH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./matrix_multiplication_cuda 100 100 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTfVMbPQ9jt1",
        "outputId": "13abc0db-ba72-4657-c2db-42784225f2aa"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU execution time: 0.043653 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./matrix_multiplication_cuda 500 500 500"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTuhCIbt9jvR",
        "outputId": "ffcfec52-b64c-4aa7-ef45-2ed316be9eff"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU execution time: 0.001274 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./matrix_multiplication_cuda 1000 1000 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LwyD8CI9jxA",
        "outputId": "c38ae261-3cbd-453e-8dd3-29e909aad71a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU execution time: 0.007127 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vKm86DT59j0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "schxtcCy9kBx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}